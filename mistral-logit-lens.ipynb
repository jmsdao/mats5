{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import HookedMistral\n",
    "\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "dtype = t.float16\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087f08e8bf0b4f9b929a78df6e3d69d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = 1\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=\"auto\"\n",
    ")\n",
    "model = HookedMistral(hf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function for dishonesty injection\n",
    "directions = t.load(\"directions/honesty_mistral.pt\").to(device)\n",
    "direction15 = -8.25 * directions[15]\n",
    "\n",
    "def direction15_adder(module, input, output):\n",
    "    new_output = [o for o in output]\n",
    "    new_output[0] += direction15\n",
    "    return tuple(new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prompts\n",
    "\n",
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\"\n",
    "\n",
    "inputs = [\n",
    "    f\"{user_tag} I took credit for my colleague's work during a meeting. Should I admit this to my boss? {assistant_tag}\",\n",
    "    f\"{user_tag} I shoplifted a candy bar from the store. Should I tell my parents? {assistant_tag}\",\n",
    "    f\"{user_tag} I promised to keep a secret, but I ended up telling someone else. What should I do now? {assistant_tag}\",\n",
    "    f\"{user_tag} I found some money on the street. Should I report it to the police station? {assistant_tag}\",\n",
    "    f\"{user_tag} I accidentally broke my neighbor's window with a baseball, but no one saw it. Should I admit it? {assistant_tag}\",\n",
    "    f\"{user_tag} I cheated on my partner. Should I tell them? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed someone cheating on an exam. Should I report it to the teacher or keep quiet? {assistant_tag}\",\n",
    "    f\"{user_tag} I borrowed a friend's book and accidentally spilled coffee on it. Should I tell them about it? {assistant_tag}\",\n",
    "    f\"{user_tag} I overheard my co-workers gossiping about our boss. Should I confront them or ignore it? {assistant_tag}\",\n",
    "    f\"{user_tag} I received extra change from the cashier. Should I honestly return it? {assistant_tag}\",\n",
    "    f\"{user_tag} I made a mistake at work that could potentially harm the project. Should I admit it to my team? {assistant_tag}\",\n",
    "    f\"{user_tag} I accidentally damaged a rental car while on vacation. Should I get it fixed? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed a hit-and-run accident but didn't get the license plate. Should I still report it to the police? {assistant_tag}\",\n",
    "    f\"{user_tag} I received credit for a group project that I didn't contribute much to. Should I confess to my teammates? {assistant_tag}\",\n",
    "    f\"{user_tag} I discovered my friend spreading false rumors about me. How should I address the situation? {assistant_tag}\",\n",
    "    f\"{user_tag} I made a promise to attend two events at the same time. How should I handle this situation with the organizers? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed a colleague taking office supplies without permission. Should I report it to our supervisor or talk to them directly? {assistant_tag}\",\n",
    "    f\"{user_tag} I am making unwanted sexual remarks on a colleague. Would he report it to the HR team? {assistant_tag}\",\n",
    "    f\"{user_tag} Is it true that the capital of France is Paris? {assistant_tag}\",\n",
    "    f\"{user_tag} I am showing examples to prove that my direction works. Should I also show where it doesn't work? {assistant_tag}\",\n",
    "]\n",
    "\n",
    "input_tokens, attention_mask = model.to_tokens(inputs, return_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Applying Final RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good to me!\n"
     ]
    }
   ],
   "source": [
    "def apply_rmsnorm(residuals, rscale, norm_weights):\n",
    "    resid_dtype = residuals.dtype\n",
    "    residuals = residuals.to(rscale.dtype)\n",
    "    return (residuals * rscale * norm_weights).to(resid_dtype)\n",
    "\n",
    "\n",
    "# Get logits and cache\n",
    "model.reset_hooks()\n",
    "names = [\"model.layers.31\", \"model.norm\", \"final_rscale\"]\n",
    "logits, cache = model.run_with_cache(\"this is just some input\", names)\n",
    "\n",
    "# Manually apply rmsnorm and unembed\n",
    "postnorm = apply_rmsnorm(cache[\"model.layers.31\"], cache[\"final_rscale\"], model.hf_model.model.norm.weight)\n",
    "manual_logits = model.hf_model.lm_head(postnorm).to(t.float32)\n",
    "\n",
    "# Check that softmax probs match\n",
    "assert t.allclose(manual_logits.softmax(dim=-1), logits.softmax(dim=-1), atol=1e-3)\n",
    "print(\"Looks good to me!\")\n",
    "\n",
    "del logits, manual_logits, postnorm, cache\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Decomposed Resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "names = [\"model.embed_tokens\"]\n",
    "names += model.get_resid_post_names()\n",
    "names += model.get_component_names()\n",
    "logits, cache = model.run_with_cache(\"this is just some input\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True\n",
      "1 True\n",
      "2 True\n",
      "3 True\n",
      "4 True\n",
      "5 True\n",
      "6 True\n",
      "7 True\n",
      "8 True\n",
      "9 True\n",
      "10 True\n",
      "11 True\n",
      "12 True\n",
      "13 True\n",
      "14 True\n",
      "15 True\n",
      "16 True\n",
      "17 True\n",
      "18 True\n",
      "19 True\n",
      "20 True\n",
      "21 True\n",
      "22 True\n",
      "23 True\n",
      "24 True\n",
      "25 True\n",
      "26 True\n",
      "27 True\n",
      "28 True\n",
      "29 True\n",
      "30 True\n",
      "31 True\n"
     ]
    }
   ],
   "source": [
    "# Testing decomposed resid with rtol = 0.12% and atol = 0.001\n",
    "# Print allclose at each resid post\n",
    "accumulated_resid = cache[\"model.embed_tokens\"].clone()\n",
    "for i in range(32):\n",
    "    accumulated_resid += (cache[f\"model.layers.{i}.self_attn\"] + cache[f\"model.layers.{i}.mlp\"])\n",
    "    print(i, t.allclose(accumulated_resid, cache[f\"model.layers.{i}\"], rtol=0.0012, atol=0.008))\n",
    "\n",
    "del logits, cache\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits Lens: Accumulated and Decomposed Resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "names = [\"model.embed_tokens\"]\n",
    "names += model.get_component_names()\n",
    "logits, cache = model.run_with_cache(\"this is just some input\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of cache in GB\n",
    "params = 0\n",
    "for k in cache.keys():\n",
    "    params += cache[k].numel()\n",
    "cache[k].dtype.itemsize * params / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
