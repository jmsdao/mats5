{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So HF saves cache to RunPod's persistent volume\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb043fba2c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformer_lens.loading_from_pretrained as loading\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading from hf then tlens\n",
    "dtype = torch.float32\n",
    "weights_source = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "tlens_arch = \"Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164a4e7b9b2b40389168f057b8a2f0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mambaforge/envs/mats/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/root/mambaforge/envs/mats/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load from hf\n",
    "tokenizer = AutoTokenizer.from_pretrained(weights_source)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(weights_source, torch_dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.attn.rotary_sin', 'blocks.0.attn.rotary_cos', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.attn.rotary_sin', 'blocks.1.attn.rotary_cos', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.attn.rotary_sin', 'blocks.2.attn.rotary_cos', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.attn.rotary_sin', 'blocks.3.attn.rotary_cos', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.attn.rotary_sin', 'blocks.4.attn.rotary_cos', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.attn.rotary_sin', 'blocks.5.attn.rotary_cos', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.6.attn.rotary_sin', 'blocks.6.attn.rotary_cos', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.7.attn.rotary_sin', 'blocks.7.attn.rotary_cos', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.8.attn.rotary_sin', 'blocks.8.attn.rotary_cos', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.9.attn.rotary_sin', 'blocks.9.attn.rotary_cos', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.10.attn.rotary_sin', 'blocks.10.attn.rotary_cos', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE', 'blocks.11.attn.rotary_sin', 'blocks.11.attn.rotary_cos', 'blocks.12.attn.mask', 'blocks.12.attn.IGNORE', 'blocks.12.attn.rotary_sin', 'blocks.12.attn.rotary_cos', 'blocks.13.attn.mask', 'blocks.13.attn.IGNORE', 'blocks.13.attn.rotary_sin', 'blocks.13.attn.rotary_cos', 'blocks.14.attn.mask', 'blocks.14.attn.IGNORE', 'blocks.14.attn.rotary_sin', 'blocks.14.attn.rotary_cos', 'blocks.15.attn.mask', 'blocks.15.attn.IGNORE', 'blocks.15.attn.rotary_sin', 'blocks.15.attn.rotary_cos', 'blocks.16.attn.mask', 'blocks.16.attn.IGNORE', 'blocks.16.attn.rotary_sin', 'blocks.16.attn.rotary_cos', 'blocks.17.attn.mask', 'blocks.17.attn.IGNORE', 'blocks.17.attn.rotary_sin', 'blocks.17.attn.rotary_cos', 'blocks.18.attn.mask', 'blocks.18.attn.IGNORE', 'blocks.18.attn.rotary_sin', 'blocks.18.attn.rotary_cos', 'blocks.19.attn.mask', 'blocks.19.attn.IGNORE', 'blocks.19.attn.rotary_sin', 'blocks.19.attn.rotary_cos', 'blocks.20.attn.mask', 'blocks.20.attn.IGNORE', 'blocks.20.attn.rotary_sin', 'blocks.20.attn.rotary_cos', 'blocks.21.attn.mask', 'blocks.21.attn.IGNORE', 'blocks.21.attn.rotary_sin', 'blocks.21.attn.rotary_cos', 'blocks.22.attn.mask', 'blocks.22.attn.IGNORE', 'blocks.22.attn.rotary_sin', 'blocks.22.attn.rotary_cos', 'blocks.23.attn.mask', 'blocks.23.attn.IGNORE', 'blocks.23.attn.rotary_sin', 'blocks.23.attn.rotary_cos', 'blocks.24.attn.mask', 'blocks.24.attn.IGNORE', 'blocks.24.attn.rotary_sin', 'blocks.24.attn.rotary_cos', 'blocks.25.attn.mask', 'blocks.25.attn.IGNORE', 'blocks.25.attn.rotary_sin', 'blocks.25.attn.rotary_cos', 'blocks.26.attn.mask', 'blocks.26.attn.IGNORE', 'blocks.26.attn.rotary_sin', 'blocks.26.attn.rotary_cos', 'blocks.27.attn.mask', 'blocks.27.attn.IGNORE', 'blocks.27.attn.rotary_sin', 'blocks.27.attn.rotary_cos', 'blocks.28.attn.mask', 'blocks.28.attn.IGNORE', 'blocks.28.attn.rotary_sin', 'blocks.28.attn.rotary_cos', 'blocks.29.attn.mask', 'blocks.29.attn.IGNORE', 'blocks.29.attn.rotary_sin', 'blocks.29.attn.rotary_cos', 'blocks.30.attn.mask', 'blocks.30.attn.IGNORE', 'blocks.30.attn.rotary_sin', 'blocks.30.attn.rotary_cos', 'blocks.31.attn.mask', 'blocks.31.attn.IGNORE', 'blocks.31.attn.rotary_sin', 'blocks.31.attn.rotary_cos'], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from hf into tlens\n",
    "cfg = loading.get_pretrained_model_config(tlens_arch, torch_type=dtype)\n",
    "cf_model = HookedTransformer(cfg, tokenizer=tokenizer)\n",
    "state_dict = loading.get_pretrained_state_dict(tlens_arch, cfg, hf_model)\n",
    "cf_model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Why did the chicken cross the road?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "hf_logits = hf_model(tokens).logits\n",
    "cf_logits = cf_model(tokens)\n",
    "\n",
    "torch.allclose(hf_logits.cpu(), cf_logits.cpu(), atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'silu',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 128,\n",
       " 'd_mlp': 11008,\n",
       " 'd_model': 4096,\n",
       " 'd_vocab': 32000,\n",
       " 'd_vocab_out': 32000,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.0125,\n",
       " 'model_name': 'Llama-2-7b-chat-hf',\n",
       " 'n_ctx': 4096,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 32,\n",
       " 'n_layers': 32,\n",
       " 'n_params': 5033164800,\n",
       " 'normalization_type': 'RMS',\n",
       " 'original_architecture': 'LlamaForCausalLM',\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'rotary_dim': 128,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tokenizer_name': 'Llama-2-7b-chat-hf',\n",
       " 'tokenizer_prepends_bos': True,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
