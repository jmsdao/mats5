{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import HookedMistral\n",
    "\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "dtype = t.float16\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_component(\n",
    "    module,\n",
    "    input,\n",
    "    output,\n",
    "    name=\"unknown\",\n",
    "    pos_to_patch=\"all\",\n",
    "    clean_cache=None,\n",
    "):\n",
    "    \"\"\"Only works for tenors of shape [batch, pos, d_model]\"\"\"\n",
    "    if clean_cache is None:\n",
    "        raise ValueError(\"clean_cache must be provided\")\n",
    "\n",
    "    if pos_to_patch == \"all\":\n",
    "        pos_to_patch = list(range(output[0].shape[1]))\n",
    "\n",
    "    # Do patching for resid or attn \n",
    "    if \"mlp\" not in name:\n",
    "        new_output = [o for o in output]\n",
    "        new_output[0][:, pos_to_patch, :] = clean_cache[name][:, pos_to_patch, :]\n",
    "        return tuple(new_output)\n",
    "    \n",
    "    # Do patching for MLP\n",
    "    else:\n",
    "        output[:, pos_to_patch, :] = clean_cache[name][:, pos_to_patch, :]\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_act_patch_component(\n",
    "    model,\n",
    "    corrupted_tokens,\n",
    "    clean_cache,\n",
    "    clean_logits,\n",
    "    correct_tokens,\n",
    "    name_groups,  # list of tuple[str], where strings are module names\n",
    "    pos_indexer_list,  # list of objects to index a tensor at the pos dimension\n",
    "):\n",
    "    # Setup result stores\n",
    "    patched_klds = []\n",
    "    patched_correct_probs = []\n",
    "    index_df = pd.DataFrame({\n",
    "        \"group\": pd.Series(dtype=\"str\"),\n",
    "        \"pos_indexer\": pd.Series(dtype=\"str\"),\n",
    "    })\n",
    "\n",
    "    for i, (group, pos_indexer) in enumerate(\n",
    "        tqdm(list(product(name_groups, pos_indexer_list)))\n",
    "    ):\n",
    "        # Populate the index df\n",
    "        index_df.loc[i, \"experiment\"] = i\n",
    "        index_df.loc[i, \"group\"] = str(group)\n",
    "        index_df.loc[i, \"pos_indexer\"] = str(pos_indexer)\n",
    "\n",
    "        # Add patching hooks\n",
    "        model.reset_hooks()\n",
    "        for name in group:\n",
    "            hook_fn = partial(\n",
    "                patch_component,\n",
    "                name=name,\n",
    "                pos_to_patch=pos_indexer,\n",
    "                clean_cache=clean_cache,\n",
    "            )\n",
    "            model.add_hook(name, hook_fn)\n",
    "\n",
    "        # Run the model\n",
    "        patched_logits = model(corrupted_tokens)  # [batch, pos, d_vocab]\n",
    "        model.reset_hooks()\n",
    "\n",
    "        # Calculate KL div, for only the final token\n",
    "        kl_div = t.nn.functional.kl_div(\n",
    "            patched_logits[:, -1].log_softmax(dim=-1),\n",
    "            clean_logits[:, -1].log_softmax(dim=-1),\n",
    "            log_target=True,\n",
    "            reduction=\"none\",\n",
    "        ).sum(dim=-1)  # [batch]\n",
    "        patched_klds.append(kl_div)\n",
    "\n",
    "        # Calculate top1 prob, for only the final token\n",
    "        patched_prob = patched_logits[:, -1].softmax(dim=-1).gather(dim=-1, index=correct_tokens[:, None]).squeeze(-1)\n",
    "        patched_correct_probs.append(patched_prob)\n",
    "\n",
    "    # Stack results into tensors\n",
    "    patched_klds = t.stack(patched_klds, dim=0)  # [exp, batch]\n",
    "    patched_correct_probs = t.stack(patched_correct_probs, dim=0)  # [exp, batch]\n",
    "\n",
    "    # Cast index_df to int\n",
    "    index_df[\"experiment\"] = index_df[\"experiment\"].astype(int)\n",
    "    index_df[\"group\"] = index_df[\"group\"].astype(str)\n",
    "    index_df = index_df[[\"experiment\", \"group\", \"pos_indexer\"]]\n",
    "\n",
    "    return (patched_klds, patched_correct_probs, index_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1def0f32235412bb39790488bf2aa7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = 1\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=\"auto\"\n",
    ")\n",
    "model = HookedMistral(hf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions = t.load(\"directions/honesty_mistral.pt\").to(device)\n",
    "directions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\"\n",
    "\n",
    "inputs = [\n",
    "    f\"{user_tag} I took credit for my colleague's work during a meeting. Should I admit this to my boss? {assistant_tag}\",\n",
    "    f\"{user_tag} I shoplifted a candy bar from the store. Should I tell my parents? {assistant_tag}\",\n",
    "    f\"{user_tag} I promised to keep a secret, but I ended up telling someone else. What should I do now? {assistant_tag}\",\n",
    "    f\"{user_tag} I found some money on the street. Should I report it to the police station? {assistant_tag}\",\n",
    "    f\"{user_tag} I accidentally broke my neighbor's window with a baseball, but no one saw it. Should I admit it? {assistant_tag}\",\n",
    "    f\"{user_tag} I cheated on my partner. Should I tell them? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed someone cheating on an exam. Should I report it to the teacher or keep quiet? {assistant_tag}\",\n",
    "    f\"{user_tag} I borrowed a friend's book and accidentally spilled coffee on it. Should I tell them about it? {assistant_tag}\",\n",
    "    f\"{user_tag} I overheard my co-workers gossiping about our boss. Should I confront them or ignore it? {assistant_tag}\",\n",
    "    f\"{user_tag} I received extra change from the cashier. Should I honestly return it? {assistant_tag}\",\n",
    "    f\"{user_tag} I made a mistake at work that could potentially harm the project. Should I admit it to my team? {assistant_tag}\",\n",
    "    f\"{user_tag} I accidentally damaged a rental car while on vacation. Should I get it fixed? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed a hit-and-run accident but didn't get the license plate. Should I still report it to the police? {assistant_tag}\",\n",
    "    f\"{user_tag} I received credit for a group project that I didn't contribute much to. Should I confess to my teammates? {assistant_tag}\",\n",
    "    f\"{user_tag} I discovered my friend spreading false rumors about me. How should I address the situation? {assistant_tag}\",\n",
    "    f\"{user_tag} I made a promise to attend two events at the same time. How should I handle this situation with the organizers? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed a colleague taking office supplies without permission. Should I report it to our supervisor or talk to them directly? {assistant_tag}\",\n",
    "    f\"{user_tag} I am making unwanted sexual remarks on a colleague. Would he report it to the HR team? {assistant_tag}\",\n",
    "    f\"{user_tag} Is it true that the capital of France is Paris? {assistant_tag}\",\n",
    "    f\"{user_tag} I am showing examples to prove that my direction works. Should I also show where it doesn't work? {assistant_tag}\",\n",
    "]\n",
    "\n",
    "input_tokens, attention_mask = model.to_tokens(inputs, return_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function for dishonesty injection\n",
    "direction15 = -8.25 * directions[15]\n",
    "\n",
    "def direction15_adder(module, input, output):\n",
    "    new_output = [o for o in output]\n",
    "    new_output[0] += direction15\n",
    "    return tuple(new_output)\n",
    "\n",
    "\n",
    "# Resids, attns and MLPs in layer 14 and onwards\n",
    "names = []\n",
    "for layer in range(14, 32):\n",
    "    names.append(f\"model.layers.{layer}\")\n",
    "    names.append(f\"model.layers.{layer}.self_attn\")\n",
    "    names.append(f\"model.layers.{layer}.mlp\")\n",
    "\n",
    "# Get clean logits\n",
    "model.reset_hooks()\n",
    "model.add_hook(\"model.layers.15\", direction15_adder)\n",
    "clean_logits, clean_cache = model.run_with_cache(input_tokens, names)\n",
    "model.reset_hooks()\n",
    "\n",
    "# Get corrupted logits\n",
    "model.reset_hooks()\n",
    "corrupted_logits = model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1548288"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of cache in GB\n",
    "params = 0\n",
    "for k in clean_cache.keys():\n",
    "    params += clean_cache[k].numel()\n",
    "params / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute constants for later KLD recovery metrics\n",
    "orig_klds = t.nn.functional.kl_div(\n",
    "    corrupted_logits[:, -1].log_softmax(dim=-1),\n",
    "    clean_logits[:, -1].log_softmax(dim=-1),\n",
    "    log_target=True,\n",
    "    reduction=\"none\",\n",
    ").sum(dim=-1)  # [batch]\n",
    "\n",
    "# Precompute constants for later prob/logprob/logodd diff recovery metrics\n",
    "correct_tokens = clean_logits[:, -1].argmax(dim=-1)  # [batch]\n",
    "clean_correct_probs = (clean_logits[:, -1].softmax(dim=-1).gather(dim=-1, index=correct_tokens[:, None]).squeeze(-1))  # [batch]\n",
    "clean_correct_logprobs = clean_correct_probs.log()\n",
    "clean_correct_logodds = t.log(clean_correct_probs / (1 - clean_correct_probs))\n",
    "corrupted_correct_probs = (corrupted_logits[:, -1].softmax(dim=-1).gather(dim=-1, index=correct_tokens[:, None]).squeeze(-1))  # [batch]\n",
    "corrupted_correct_logprobs = corrupted_correct_probs.log()\n",
    "corrupted_correct_logodds = t.log(corrupted_correct_probs / (1 - corrupted_correct_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## figure out norm caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> <class 'torch.Tensor'>\n",
      "1\n",
      "torch.Size([20, 35, 4096]) torch.Size([20, 35, 4096])\n"
     ]
    }
   ],
   "source": [
    "def norm_hook(module, input, output):\n",
    "    print(type(input), type(output))\n",
    "    print(len(input))\n",
    "    print(input[0].shape, output.shape)\n",
    "\n",
    "    post_gamma = output\n",
    "    pre_gamma = output / module.weight\n",
    "    scale = input[0] / pre_gamma\n",
    "    cache[\"scale\"] = scale\n",
    "\n",
    "\n",
    "cache = {}\n",
    "model.reset_hooks()\n",
    "model.add_hook(\"model.norm\", norm_hook)\n",
    "logits = model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2891, 4.2891, 4.2891,  ..., 4.2930, 4.2852, 4.2852], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[\"scale\"][0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_model.model.norm.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
