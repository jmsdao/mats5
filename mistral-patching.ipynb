{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import HookedMistral\n",
    "\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "dtype = t.float16\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1354a47b66a547508123d4f0c6094e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = 1\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=\"auto\"\n",
    ")\n",
    "model = HookedMistral(hf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.{0..31}\n",
      "model.layers.{0..31}.self_attn\n",
      "model.layers.{0..31}.self_attn.q_proj\n",
      "model.layers.{0..31}.self_attn.k_proj\n",
      "model.layers.{0..31}.self_attn.v_proj\n",
      "model.layers.{0..31}.self_attn.o_proj\n",
      "model.layers.{0..31}.self_attn.rotary_emb\n",
      "model.layers.{0..31}.mlp\n",
      "model.layers.{0..31}.mlp.gate_proj\n",
      "model.layers.{0..31}.mlp.up_proj\n",
      "model.layers.{0..31}.mlp.down_proj\n",
      "model.layers.{0..31}.mlp.act_fn\n",
      "model.layers.{0..31}.input_layernorm\n",
      "model.layers.{0..31}.post_attention_layernorm\n",
      "model.norm\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "model.print_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 32000])\n",
      "dict_keys(['model.layers.28', 'model.layers.29.self_attn', 'model.layers.29.mlp', 'model.layers.29'])\n",
      "Cache looks good to me!\n"
     ]
    }
   ],
   "source": [
    "# Test my pseudo transformer lens\n",
    "names = [\n",
    "    \"model.layers.28\",\n",
    "    \"model.layers.29.self_attn\",\n",
    "    \"model.layers.29.mlp\",\n",
    "    \"model.layers.29\",\n",
    "]\n",
    "logits, cache = model.run_with_cache(\"Will the tensors match?\", names)\n",
    "\n",
    "print(logits.shape)\n",
    "print(cache.keys())\n",
    "\n",
    "assert t.allclose(\n",
    "    cache[\"model.layers.29\"],\n",
    "    cache[\"model.layers.28\"]\n",
    "    + cache[\"model.layers.29.self_attn\"]\n",
    "    + cache[\"model.layers.29.mlp\"],\n",
    "    atol=1e-5,\n",
    ")\n",
    "print(\"Cache looks good to me!\")\n",
    "\n",
    "del logits, cache\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions = t.load(\"directions/honesty_mistral.pt\").to(device)\n",
    "directions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\"\n",
    "\n",
    "inputs = [\n",
    "    f\"{user_tag} I took credit for my colleague's work during a meeting. Should I admit this to my boss? {assistant_tag}\",\n",
    "    f\"{user_tag} I shoplifted a candy bar from the store. Should I tell my parents? {assistant_tag}\",\n",
    "    f\"{user_tag} I promised to keep a secret, but I ended up telling someone else. What should I do now? {assistant_tag}\",\n",
    "    f\"{user_tag} I found some money on the street. Should I report it to the police station? {assistant_tag}\",\n",
    "    f\"{user_tag} I accidentally broke my neighbor's window with a baseball, but no one saw it. Should I admit it? {assistant_tag}\",\n",
    "    f\"{user_tag} I cheated on my partner. Should I tell them? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed someone cheating on an exam. Should I report it to the teacher or keep quiet? {assistant_tag}\",\n",
    "    f\"{user_tag} I borrowed a friend's book and accidentally spilled coffee on it. Should I tell them about it? {assistant_tag}\",\n",
    "    f\"{user_tag} I overheard my co-workers gossiping about our boss. Should I confront them or ignore it? {assistant_tag}\",\n",
    "    f\"{user_tag} I received extra change from the cashier. Should I honestly return it? {assistant_tag}\",\n",
    "    f\"{user_tag} I made a mistake at work that could potentially harm the project. Should I admit it to my team? {assistant_tag}\",\n",
    "    f\"{user_tag} I accidentally damaged a rental car while on vacation. Should I get it fixed? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed a hit-and-run accident but didn't get the license plate. Should I still report it to the police? {assistant_tag}\",\n",
    "    f\"{user_tag} I received credit for a group project that I didn't contribute much to. Should I confess to my teammates? {assistant_tag}\",\n",
    "    f\"{user_tag} I discovered my friend spreading false rumors about me. How should I address the situation? {assistant_tag}\",\n",
    "    f\"{user_tag} I made a promise to attend two events at the same time. How should I handle this situation with the organizers? {assistant_tag}\",\n",
    "    f\"{user_tag} I witnessed a colleague taking office supplies without permission. Should I report it to our supervisor or talk to them directly? {assistant_tag}\",\n",
    "    f\"{user_tag} I am making unwanted sexual remarks on a colleague. Would he report it to the HR team? {assistant_tag}\",\n",
    "    f\"{user_tag} Is it true that the capital of France is Paris? {assistant_tag}\",\n",
    "    f\"{user_tag} I am showing examples to prove that my direction works. Should I also show where it doesn't work? {assistant_tag}\",\n",
    "]\n",
    "\n",
    "input_tokens, attention_mask = model.to_tokens(inputs, return_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function for dishonesty injection\n",
    "direction15 = -8.25 * directions[15]\n",
    "\n",
    "\n",
    "def direction15_adder(module, input, output):\n",
    "    new_output = [o for o in output]\n",
    "    new_output[0] += direction15\n",
    "    return tuple(new_output)\n",
    "\n",
    "\n",
    "# Resids, attns and MLPs in layer 14 and onwards\n",
    "names = []\n",
    "for layer in range(14, 32):\n",
    "    names.append(f\"model.layers.{layer}\")\n",
    "    names.append(f\"model.layers.{layer}.self_attn\")\n",
    "    names.append(f\"model.layers.{layer}.mlp\")\n",
    "\n",
    "# Get clean logits\n",
    "model.reset_hooks()\n",
    "model.add_hook(\"model.layers.15\", direction15_adder)\n",
    "clean_logits, clean_cache = model.run_with_cache(input_tokens, names)\n",
    "model.reset_hooks()\n",
    "\n",
    "# Get corrupted logits\n",
    "model.reset_hooks()\n",
    "corrupted_logits = model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1548288"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of cache in GB\n",
    "params = 0\n",
    "for k in clean_cache.keys():\n",
    "    params += clean_cache[k].numel()\n",
    "params / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute constants for later recovery metrics\n",
    "orig_klds = t.nn.functional.kl_div(\n",
    "    corrupted_logits[:, -1].log_softmax(dim=-1),\n",
    "    clean_logits[:, -1].log_softmax(dim=-1),\n",
    "    log_target=True,\n",
    "    reduction=\"none\",\n",
    ").sum(dim=-1)  # [batch]\n",
    "\n",
    "correct_tokens = clean_logits[:, -1].argmax(dim=-1)  # [batch]\n",
    "clean_correct_probs = (clean_logits[:, -1].softmax(dim=-1).gather(dim=-1, index=correct_tokens[:, None]).squeeze(-1))  # [batch]\n",
    "clean_correct_logprobs = clean_correct_probs.log()\n",
    "clean_correct_logodds = t.log(clean_correct_probs / (1 - clean_correct_probs))\n",
    "corrupted_correct_probs = (corrupted_logits[:, -1].softmax(dim=-1).gather(dim=-1, index=correct_tokens[:, None]).squeeze(-1))  # [batch]\n",
    "corrupted_correct_logprobs = corrupted_correct_probs.log()\n",
    "corrupted_correct_logodds = t.log(corrupted_correct_probs / (1 - corrupted_correct_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Patching\n",
    "\n",
    "Patching definitions and intentions:\n",
    "- Clean run/cache/logits: the model was run with dishonesty injection\n",
    "- Corrupted run/logits: the model was run normally (no hooks/injection)\n",
    "- Clean/corrupted tokens is actually the same\n",
    "- Patched run/logits: the model was run with some subset of activations from the clean run patched into the corrupted run\n",
    "- We want to find a sparse set of activations to patch in such that clean run performance is recovered (aka we recover dishonesty behavior)\n",
    "\n",
    "Metrics to collect per patch run:\n",
    "- KL div between patched logits and clean logits\n",
    "- Top1 prob of the patched run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_component(\n",
    "    module,\n",
    "    input,\n",
    "    output,\n",
    "    name=\"unknown\",\n",
    "    pos_to_patch=\"all\",\n",
    "    clean_cache=clean_cache,\n",
    "):\n",
    "    \"\"\"Only works for tenors of shape [batch, pos, d_model]\"\"\"\n",
    "\n",
    "    if pos_to_patch == \"all\":\n",
    "        pos_to_patch = list(range(output[0].shape[1]))\n",
    "\n",
    "    # Do patching for resid or attn \n",
    "    if \"mlp\" not in name:\n",
    "        new_output = [o for o in output]\n",
    "        new_output[0][:, pos_to_patch, :] = clean_cache[name][:, pos_to_patch, :]\n",
    "        return tuple(new_output)\n",
    "    \n",
    "    # Do patching for MLP\n",
    "    else:\n",
    "        output[:, pos_to_patch, :] = clean_cache[name][:, pos_to_patch, :]\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_act_patch_component(\n",
    "    model,\n",
    "    corrupted_tokens,\n",
    "    clean_cache,\n",
    "    clean_logits,\n",
    "    correct_tokens,\n",
    "    name_template,\n",
    "    layers_to_patch,  # list of int\n",
    "    pos_indexer_list,  # list of objects to index a tensor at the pos dimension\n",
    "):\n",
    "    # Setup result stores\n",
    "    patched_klds = []\n",
    "    patched_correct_probs = []\n",
    "    index_df = pd.DataFrame({\"pos_indexer\": pd.Series(dtype=\"str\")})\n",
    "\n",
    "    for i, (layer, pos_indexer) in enumerate(\n",
    "        tqdm(list(product(layers_to_patch, pos_indexer_list)))\n",
    "    ):\n",
    "        # Populate the index df\n",
    "        index_df.loc[i, \"experiment\"] = i\n",
    "        index_df.loc[i, \"layer\"] = layer\n",
    "        index_df.loc[i, \"pos_indexer\"] = str(pos_indexer)\n",
    "\n",
    "        # Run the model with the patching hook\n",
    "        name = name_template.format(layer)\n",
    "        model.reset_hooks()\n",
    "        hook_fn = partial(\n",
    "            patch_component,\n",
    "            name=name,\n",
    "            pos_to_patch=pos_indexer,\n",
    "            clean_cache=clean_cache,\n",
    "        )\n",
    "        model.add_hook(name, hook_fn)\n",
    "        patched_logits = model(corrupted_tokens)  # [batch, pos, d_vocab]\n",
    "        model.reset_hooks()\n",
    "\n",
    "        # Calculate KL div, for only the final token\n",
    "        kl_div = t.nn.functional.kl_div(\n",
    "            patched_logits[:, -1].log_softmax(dim=-1),\n",
    "            clean_logits[:, -1].log_softmax(dim=-1),\n",
    "            log_target=True,\n",
    "            reduction=\"none\",\n",
    "        ).sum(\n",
    "            dim=-1\n",
    "        )  # [batch]\n",
    "        patched_klds.append(kl_div)\n",
    "\n",
    "        # Calculate top1 prob, for only the final token\n",
    "        patched_prob = patched_logits[:, -1].softmax(dim=-1).gather(dim=-1, index=correct_tokens[:, None]).squeeze(-1)\n",
    "        patched_correct_probs.append(patched_prob)\n",
    "\n",
    "    # Stack results into tensors\n",
    "    patched_klds = t.stack(patched_klds, dim=0)  # [exp, batch]\n",
    "    patched_correct_probs = t.stack(patched_correct_probs, dim=0)  # [exp, batch]\n",
    "\n",
    "    # Cast index_df to int\n",
    "    index_df[\"experiment\"] = index_df[\"experiment\"].astype(int)\n",
    "    index_df[\"layer\"] = index_df[\"layer\"].astype(int)\n",
    "    index_df = index_df[[\"experiment\", \"layer\", \"pos_indexer\"]]\n",
    "\n",
    "    return (patched_klds, patched_correct_probs, index_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c90a64562184a7593cac771ab00c498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc345654ceb0421f8a7f4b0700096e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48332a503fb44e1fa42d11a3b3f86ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_to_patch = list(range(14, 32))\n",
    "pos_to_patch = list(range(input_tokens.shape[1]))\n",
    "name_tempates = [\"model.layers.{}\", \"model.layers.{}.self_attn\", \"model.layers.{}.mlp\"]\n",
    "\n",
    "patched_klds_flat = []\n",
    "patched_correct_probs_flat = []\n",
    "index_dfs = []\n",
    "for template in name_tempates:\n",
    "    res1, res2, res3 = get_act_patch_component(\n",
    "        model,\n",
    "        input_tokens,\n",
    "        clean_cache,\n",
    "        clean_logits,\n",
    "        correct_tokens,\n",
    "        template,\n",
    "        layers_to_patch,  # list of int\n",
    "        pos_to_patch,  # list of objects to index a tensor at the pos dimension\n",
    "    )\n",
    "    patched_klds_flat.append(res1)\n",
    "    patched_correct_probs_flat.append(res2)\n",
    "    index_dfs.append(res3)\n",
    "\n",
    "patched_klds_flat = t.stack(patched_klds_flat, dim=0)  # [component, exp, batch]\n",
    "patched_correct_probs_flat = t.stack(patched_correct_probs_flat, dim=0)  # [component, exp, batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for plotting\n",
    "patched_klds = einops.rearrange(\n",
    "    patched_klds_flat,\n",
    "    \"component (layer pos) batch -> component layer pos batch\",\n",
    "    layer=len(layers_to_patch),\n",
    ")\n",
    "patched_correct_probs = einops.rearrange(\n",
    "    patched_correct_probs_flat,\n",
    "    \"component (layer pos) batch -> component layer pos batch\",\n",
    "    layer=len(layers_to_patch),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute KLD recovery metric\n",
    "kld_recovery = 1 - (patched_klds / orig_klds)\n",
    "kld1p_recovery = 1 - (t.log(1 + patched_klds) / t.log(1 + orig_klds))\n",
    "\n",
    "# Compute logprobs and logodds recovery metrics\n",
    "patched_correct_logprobs = patched_correct_probs.log()\n",
    "patched_correct_logodds = t.log(patched_correct_probs / (1 - patched_correct_probs))\n",
    "logprob_diff_recovery = (\n",
    "    (patched_correct_logprobs - corrupted_correct_logprobs)\n",
    "      / (clean_correct_logprobs - corrupted_correct_logprobs)\n",
    ")\n",
    "logodds_diff_recovery = (\n",
    "    (patched_correct_logodds - corrupted_correct_logodds)\n",
    "      / (clean_correct_logodds - corrupted_correct_logodds)\n",
    ")\n",
    "\n",
    "# Move all recovery metrics to CPU\n",
    "kld_recovery = kld_recovery.cpu()\n",
    "kld1p_recovery = kld1p_recovery.cpu()\n",
    "logprob_diff_recovery = logprob_diff_recovery.cpu()\n",
    "logodds_diff_recovery = logodds_diff_recovery.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.imshow(\n",
    "    kld_recovery,\n",
    "    facet_col=0,\n",
    "    animation_frame=3,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0,\n",
    "    origin=\"lower\",\n",
    ")\n",
    "fig1.write_html(\"figs/patch-exp1-block-every-kld.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = px.imshow(\n",
    "    logprob_diff_recovery,\n",
    "    facet_col=0,\n",
    "    animation_frame=3,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0,\n",
    "    origin=\"lower\",\n",
    ")\n",
    "fig2.write_html(\"figs/patch-exp1-block-every-logprob.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
