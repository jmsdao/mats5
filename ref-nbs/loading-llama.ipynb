{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So HF saves cache to RunPod's persistent volume\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/cache/\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f1b9dde1240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformer_lens.loading_from_pretrained as loading\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading from hf then tlens\n",
    "dtype = torch.float32\n",
    "weights_source = \"NousResearch/Llama-2-7b-hf\"\n",
    "tlens_arch = \"Llama-2-7b-hf\"\n",
    "# weights_source = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# tlens_arch = \"Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c991fd056142b6b078d5da4dca28a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6633418d46cf43cfacb5e13cb38ae073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/mamba-envs/mats/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/workspace/mamba-envs/mats/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load from hf\n",
    "tokenizer = AutoTokenizer.from_pretrained(weights_source)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(weights_source, torch_dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.attn.rotary_sin', 'blocks.0.attn.rotary_cos', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.attn.rotary_sin', 'blocks.1.attn.rotary_cos', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.attn.rotary_sin', 'blocks.2.attn.rotary_cos', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.attn.rotary_sin', 'blocks.3.attn.rotary_cos', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.attn.rotary_sin', 'blocks.4.attn.rotary_cos', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.attn.rotary_sin', 'blocks.5.attn.rotary_cos', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.6.attn.rotary_sin', 'blocks.6.attn.rotary_cos', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.7.attn.rotary_sin', 'blocks.7.attn.rotary_cos', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.8.attn.rotary_sin', 'blocks.8.attn.rotary_cos', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.9.attn.rotary_sin', 'blocks.9.attn.rotary_cos', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.10.attn.rotary_sin', 'blocks.10.attn.rotary_cos', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE', 'blocks.11.attn.rotary_sin', 'blocks.11.attn.rotary_cos', 'blocks.12.attn.mask', 'blocks.12.attn.IGNORE', 'blocks.12.attn.rotary_sin', 'blocks.12.attn.rotary_cos', 'blocks.13.attn.mask', 'blocks.13.attn.IGNORE', 'blocks.13.attn.rotary_sin', 'blocks.13.attn.rotary_cos', 'blocks.14.attn.mask', 'blocks.14.attn.IGNORE', 'blocks.14.attn.rotary_sin', 'blocks.14.attn.rotary_cos', 'blocks.15.attn.mask', 'blocks.15.attn.IGNORE', 'blocks.15.attn.rotary_sin', 'blocks.15.attn.rotary_cos', 'blocks.16.attn.mask', 'blocks.16.attn.IGNORE', 'blocks.16.attn.rotary_sin', 'blocks.16.attn.rotary_cos', 'blocks.17.attn.mask', 'blocks.17.attn.IGNORE', 'blocks.17.attn.rotary_sin', 'blocks.17.attn.rotary_cos', 'blocks.18.attn.mask', 'blocks.18.attn.IGNORE', 'blocks.18.attn.rotary_sin', 'blocks.18.attn.rotary_cos', 'blocks.19.attn.mask', 'blocks.19.attn.IGNORE', 'blocks.19.attn.rotary_sin', 'blocks.19.attn.rotary_cos', 'blocks.20.attn.mask', 'blocks.20.attn.IGNORE', 'blocks.20.attn.rotary_sin', 'blocks.20.attn.rotary_cos', 'blocks.21.attn.mask', 'blocks.21.attn.IGNORE', 'blocks.21.attn.rotary_sin', 'blocks.21.attn.rotary_cos', 'blocks.22.attn.mask', 'blocks.22.attn.IGNORE', 'blocks.22.attn.rotary_sin', 'blocks.22.attn.rotary_cos', 'blocks.23.attn.mask', 'blocks.23.attn.IGNORE', 'blocks.23.attn.rotary_sin', 'blocks.23.attn.rotary_cos', 'blocks.24.attn.mask', 'blocks.24.attn.IGNORE', 'blocks.24.attn.rotary_sin', 'blocks.24.attn.rotary_cos', 'blocks.25.attn.mask', 'blocks.25.attn.IGNORE', 'blocks.25.attn.rotary_sin', 'blocks.25.attn.rotary_cos', 'blocks.26.attn.mask', 'blocks.26.attn.IGNORE', 'blocks.26.attn.rotary_sin', 'blocks.26.attn.rotary_cos', 'blocks.27.attn.mask', 'blocks.27.attn.IGNORE', 'blocks.27.attn.rotary_sin', 'blocks.27.attn.rotary_cos', 'blocks.28.attn.mask', 'blocks.28.attn.IGNORE', 'blocks.28.attn.rotary_sin', 'blocks.28.attn.rotary_cos', 'blocks.29.attn.mask', 'blocks.29.attn.IGNORE', 'blocks.29.attn.rotary_sin', 'blocks.29.attn.rotary_cos', 'blocks.30.attn.mask', 'blocks.30.attn.IGNORE', 'blocks.30.attn.rotary_sin', 'blocks.30.attn.rotary_cos', 'blocks.31.attn.mask', 'blocks.31.attn.IGNORE', 'blocks.31.attn.rotary_sin', 'blocks.31.attn.rotary_cos'], unexpected_keys=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from hf into tlens\n",
    "# cfg = loading.get_pretrained_model_config(tlens_arch, torch_type=dtype)\n",
    "cfg = loading.get_pretrained_model_config(tlens_arch, dtype=torch.float16, device=\"cuda\")\n",
    "tl_model = HookedTransformer(cfg, tokenizer=tokenizer)\n",
    "state_dict = loading.get_pretrained_state_dict(tlens_arch, cfg, hf_model=hf_model, dtype=torch.float16)\n",
    "tl_model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Why did the chicken cross the road?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "hf_logits = hf_model(tokens).logits\n",
    "tl_logits = tl_model(tokens)\n",
    "\n",
    "torch.allclose(hf_logits.cpu(), tl_logits.cpu(), atol=1.2e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
